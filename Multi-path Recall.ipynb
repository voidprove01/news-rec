{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28625ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import os, math, warnings, math, pickle\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "import faiss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fb1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data_raw/'\n",
    "save_path = './temp_results/'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "metric_recall = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006212b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug mode\n",
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    \"\"\"\n",
    "        subsample data for testing purposes\n",
    "        data_path:\n",
    "        sample_nums: sample size\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False) \n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click\n",
    "\n",
    "# get click data，online: train data+test data\n",
    "# offlilne: train data only\n",
    "def get_all_click_df(data_path='./data_raw/', offline=True):\n",
    "    if offline:\n",
    "        all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    else:\n",
    "        trn_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "\n",
    "        all_click = pd.concat([trn_click, tst_click], ignore_index=True)\n",
    "\n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c02b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading basic item info\n",
    "def get_item_info_df(data_path):\n",
    "    item_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    \n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    \n",
    "    return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35db56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading article embedding data\n",
    "def get_item_emb_dict(data_path):\n",
    "    item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "    \n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "    # normalization\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "    pickle.dump(item_emb_dict, open(save_path + 'item_content_emb.pkl', 'wb'))\n",
    "    \n",
    "    return item_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c0ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e5baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "# all_click_df = get_all_click_sample(data_path)\n",
    "\n",
    "# full data\n",
    "all_click_df = get_all_click_df(offline=False)\n",
    "\n",
    "# Normalize the click_timestamp column\n",
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055d90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_info_df = get_item_info_df(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3201f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_dict = get_item_emb_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576df5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  creste a dict where each user maps to a dict of items and their clike times  {user1: {item1: time1, item2: time2..}...}\n",
    "def get_user_item_time(click_df):\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    \n",
    "    def make_item_time_pair(df):\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "    \n",
    "    user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']].apply(lambda x: make_item_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "    \n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71f3b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict where where each item maps to a dictionary of users and their click times:  {item1: {user1: time1, user2: time2...}...}\n",
    "def get_item_user_time_dict(click_df):\n",
    "    def make_user_time_pair(df):\n",
    "        return list(zip(df['user_id'], df['click_timestamp']))\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    item_user_time_df = click_df.groupby('click_article_id')[['user_id', 'click_timestamp']].apply(lambda x: make_user_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'user_time_list'})\n",
    "    \n",
    "    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "    return item_user_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630d9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two dataframes: one contains all history clicks and the other one contains the last click for each user\n",
    "def get_hist_and_last_click(all_click):\n",
    "    \n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    #  If a user has only one click, their historical data will default to containing that single click to prevent the user from becoming \"invisible\" during training\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52906cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting item info and save them as a dictionary\n",
    "def get_item_info_dict(item_info_df):\n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "    \n",
    "    return item_type_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d55cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_hist_item_info_dict(all_click):\n",
    "    \n",
    "    # group items by their user_id and create a dictionary that maps each user to the set of category_id values they have interacted with\n",
    "    user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "    user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n",
    "    \n",
    "    # create a dictionary that maps each user to the set of articles they clicked\n",
    "    user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "    \n",
    "    # create a dictionary that maps each user to average words count\n",
    "    user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "    \n",
    "    # create a dictionary that maps each user to the his last clicktimestamp\n",
    "    all_click_ = all_click.sort_values('click_timestamp')\n",
    "    user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    \n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "    \n",
    "    return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab4fd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get items with top k clicks\n",
    "def get_item_topk_click(click_df, k):\n",
    "    topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "    return topk_click"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795c499",
   "metadata": {},
   "source": [
    "## Define a multi-channel recall dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6a04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get article info\n",
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aadf6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a multi-channel recall dictionary to store the results of various recall methods\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'cold_start_recall': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d240dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting last one click for recall performance evaluation\n",
    "trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc5e31",
   "metadata": {},
   "source": [
    "## Metric to evaluation recall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42385e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the hit rate at different cutoffs (e.g., top-10, top-20, top-30, top-40, and top-50 items)\n",
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=5):\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "    user_num = len(user_recall_items_dict)\n",
    "    \n",
    "    for k in range(10, topk+1, 10):\n",
    "        hit_num = 0\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            # get tok k items\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1\n",
    "        \n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "        print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', user_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02f0d5",
   "metadata": {},
   "source": [
    "## Computation of collaborative-filtering similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7b0003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item to item similarity matrix\n",
    "def itemcf_sim(df, item_created_time_dict):\n",
    "    \"\"\"\n",
    "        item to item similarity\n",
    "        :param df: dataset\n",
    "        :item_created_time_dict:  \n",
    "        return : item to item similarity matrix\n",
    "        The function implements item-based collaborative filtering (ItemCF) enhanced with additional weights\n",
    "    \"\"\"\n",
    "    \n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # \n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                    \n",
    "                # Assigns higher importance to items clicked close together.\n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # Penalizes pairs that were clicked far apart in time.\n",
    "                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                # Penalizes or rewards items based on how similar their creation times are.\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                # normalize to prevents users with long click sequences from disproportionately influencing the similarity score.\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "                \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33cc4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 250000/250000 [01:43<00:00, 2419.74it/s]\n"
     ]
    }
   ],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa47b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cd3d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user to user similarity\n",
    "def get_user_activate_degree_dict(all_click_df):\n",
    "    all_click_df_ = all_click_df.groupby('user_id')['click_article_id'].count().reset_index()\n",
    "    \n",
    "    # normalization\n",
    "    mm = MinMaxScaler()\n",
    "    all_click_df_['click_article_id'] = mm.fit_transform(all_click_df_[['click_article_id']])\n",
    "    user_activate_degree_dict = dict(zip(all_click_df_['user_id'], all_click_df_['click_article_id']))\n",
    "    \n",
    "    return user_activate_degree_dict\n",
    "\n",
    "def usercf_sim(all_click_df, user_activate_degree_dict):\n",
    "    \"\"\"\n",
    "        :param all_click_df: dataframe\n",
    "        :param user_activate_degree_dict\n",
    "        return user-based similarity matrix\n",
    "        \n",
    "        user-based collaborative filtering matrix computation\n",
    "        \n",
    "    \"\"\"\n",
    "    item_user_time_dict = get_item_user_time_dict(all_click_df)\n",
    "    \n",
    "    u2u_sim = {}\n",
    "    user_cnt = defaultdict(int)\n",
    "    for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "        for u, click_time in user_time_list:\n",
    "            user_cnt[u] += 1\n",
    "            u2u_sim.setdefault(u, {})\n",
    "            for v, click_time in user_time_list:\n",
    "                u2u_sim[u].setdefault(v, 0)\n",
    "                if u == v:\n",
    "                    continue\n",
    "                # activity weight\n",
    "                activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])   \n",
    "                u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)\n",
    "    \n",
    "    u2u_sim_ = u2u_sim.copy()\n",
    "    for u, related_users in u2u_sim.items():\n",
    "        for v, wij in related_users.items():\n",
    "            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])\n",
    "    \n",
    "    # save to local file\n",
    "    pickle.dump(u2u_sim_, open(save_path + 'usercf_u2u_sim.pkl', 'wb'))\n",
    "\n",
    "    return u2u_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b29105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too much storage required to compute u2u_sim so halt the task here\n",
    "# run on sample data\n",
    "#user_activate_degree_dict = get_user_activate_degree_dict(all_click_df)\n",
    "#u2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e805694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item embedding similarity using Faiss\n",
    "# Faiss search returns the top k item based on similarty score for each item\n",
    "def embdding_sim(click_df, item_emb_df, save_path, topk):\n",
    "    \"\"\"\n",
    "        Item similarity calculation based on content-based embedding\n",
    "        :param click_df: dataframe\n",
    "        :param item_emb_df: embedding of articles\n",
    "        :param save_path: save path\n",
    "        :patam topk: top k most similar\n",
    "        return dictionary\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # mapping item index to raw id \n",
    "    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df['article_id']))\n",
    "    \n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n",
    "    # vector normilization\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "    \n",
    "    # construct faiss index\n",
    "    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])\n",
    "    item_index.add(item_emb_np)\n",
    "    # similarity search，return top k items and respective similarity score for every index\n",
    "    sim, idx = item_index.search(item_emb_np, topk) # return a list\n",
    "    \n",
    "    # saving search results with corresponding raw ID\n",
    "    item_sim_dict = collections.defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n",
    "        target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "        # starting from index 1 to exclude the item itself\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]): \n",
    "            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "    \n",
    "    # save i2i similarity matrix\n",
    "    pickle.dump(item_sim_dict, open(save_path + 'emb_i2i_sim.pkl', 'wb'))   \n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d94df276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364047it [00:04, 87063.15it/s]\n"
     ]
    }
   ],
   "source": [
    "item_emb_df = pd.read_csv(data_path + '/articles_emb.csv')\n",
    "emb_i2i_sim = embdding_sim(all_click_df, item_emb_df, save_path, topk=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4f51f",
   "metadata": {},
   "source": [
    "# Recall generation using item to item similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc60d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "        recall based on item CF\n",
    "        :param user_id: user id\n",
    "        :param user_item_time_dict: dict   {user1: {item1: time1, item2: time2..}...}\n",
    "        :param i2i_sim: dict \n",
    "        :param sim_item_topk: int\n",
    "        :param recall_item_num: int\n",
    "        :param item_topk_click: list of most popular articles to fill the individual user's recalled list in case there is vacancy\n",
    "        :param emb_i2i_sim: dict\n",
    "        \n",
    "        return: recall article dict {item1:score1, item2: score2...}\n",
    "        \n",
    "    \"\"\"\n",
    "    # get user historical interacted items\n",
    "    user_hist_items = user_item_time_dict[user_id]\n",
    "    \n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "            if j in user_hist_items:\n",
    "                continue\n",
    "            \n",
    "            # weight for difference in article cerated time\n",
    "            created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "            # Applies a positional weight based on how recent the interaction with item i is\n",
    "            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n",
    "            \n",
    "            content_weight = 1.0\n",
    "            if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                content_weight += emb_i2i_sim[i][j]\n",
    "            if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                content_weight += emb_i2i_sim[j][i]\n",
    "                \n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += created_time_weight * loc_weight * content_weight * wij\n",
    "    \n",
    "    # fill the item list with popular items if fewer than 10\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items(): # item to be filled should not be in the original list\n",
    "                continue\n",
    "            item_rank[item] = - i - 100 # any negative number\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "    \n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "        \n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bfbb6",
   "metadata": {},
   "source": [
    "## Recall using both i2iCF and item embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1f347f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 250000/250000 [29:17<00:00, 142.21it/s]\n"
     ]
    }
   ],
   "source": [
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "emb_i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "sim_item_topk = 20\n",
    "recall_item_num = 10\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \\\n",
    "                                                        i2i_sim, sim_item_topk, recall_item_num, \\\n",
    "                                                        item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['itemcf_sim_itemcf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['itemcf_sim_itemcf_recall'], open(save_path + 'itemcf_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    #\n",
    "    metrics_recall(user_multi_recall_dict['itemcf_sim_itemcf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f12bf4",
   "metadata": {},
   "source": [
    "## Recall generation using item embedding only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35931fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 250000/250000 [00:32<00:00, 7800.57it/s]\n"
     ]
    }
   ],
   "source": [
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df#extract last click for evaluation\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 20\n",
    "recall_item_num = 10\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, \n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "    \n",
    "user_multi_recall_dict['embedding_sim_item_recall'] = user_recall_items_dict#saving into the multi recall dictionary\n",
    "pickle.dump(user_multi_recall_dict['embedding_sim_item_recall'], open(save_path + 'embedding_sim_item_recall.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # recall performance evaluation\n",
    "    metrics_recall(user_multi_recall_dict['embedding_sim_item_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca57f8d",
   "metadata": {},
   "source": [
    "# Recall Generation using YouTube DNN, aka Two Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "438d4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for training two-tower model\n",
    "\n",
    "def gen_data_set(data, negsample=0):\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    item_ids = data['click_article_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "        \n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))   # select neg sample from unread articles\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)  # select n negsamples for every positive sample\n",
    "            \n",
    "        # For users with only one interaction, add that single interaction to both the training and test sets to prevent missing embeddings during training.\n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((reviewerID, [pos_list[0]], pos_list[0],1,len(pos_list)))\n",
    "            test_set.append((reviewerID, [pos_list[0]], pos_list[0],1,len(pos_list)))\n",
    "            \n",
    "        # construct sample using sliding window\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1])))  # 正样本 [user_id, his_item, pos_item, label, len(his_item)]\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1]))) # 负样本 [user_id, his_item, neg_item, label, len(his_item)]\n",
    "            else:\n",
    "                # making the longest sequence as test data\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1])))\n",
    "                \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "# padding intput data to ensure consistent input sizes\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"click_article_id\": train_iid, \"hist_article_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b3a97",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf2d670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, Flatten, Concatenate\n",
    "\n",
    "# Input layers\n",
    "user_id_input = Input(shape=(1,), name=\"user_id\")\n",
    "click_article_id_input = Input(shape=(1,), name=\"click_article_id\")\n",
    "hist_article_id_input = Input(shape=(10,), name=\"hist_article_id\")  # Example max_len=10\n",
    "hist_len_input = Input(shape=(1,), name=\"hist_len\")\n",
    "\n",
    "# Embedding layers\n",
    "embedding_dim = 16\n",
    "user_embedding = Embedding(input_dim=250000, output_dim=embedding_dim, name=\"user_embedding\")(user_id_input)\n",
    "item_embedding = Embedding(input_dim=35380, output_dim=embedding_dim, name=\"item_embedding\")(click_article_id_input)\n",
    "hist_embedding = Embedding(input_dim=35380, output_dim=embedding_dim, name=\"hist_embedding\")(hist_article_id_input)\n",
    "\n",
    "# GRU for sequence modeling\n",
    "gru_out = GRU(units=16, return_sequences=False)(hist_embedding)\n",
    "\n",
    "# Combine embeddings\n",
    "concat = Concatenate()([Flatten()(user_embedding), Flatten()(item_embedding), gru_out])\n",
    "\n",
    "# Dense layers for prediction\n",
    "dense_out = Dense(units=64, activation=\"relu\")(concat)\n",
    "output = Dense(units=1, activation=\"sigmoid\")(dense_out)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs={\"user_id\":user_id_input,\"click_article_id\":click_article_id_input,\"hist_article_id\":hist_article_id_input,\"hist_len\": hist_len_input},outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3211e",
   "metadata": {},
   "source": [
    "## Formatting input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90d4f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 250000/250000 [00:08<00:00, 30124.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# split train data and test data\n",
    "SEQ_LEN = 10 # length of the user click sequence\n",
    "user_profile_ = all_click_df[[\"user_id\"]].drop_duplicates('user_id')\n",
    "item_profile_ = all_click_df[[\"click_article_id\"]].drop_duplicates('click_article_id')  \n",
    "# Catogorical encoding\n",
    "features = [\"click_article_id\", \"user_id\"]\n",
    "feature_max_idx = {}\n",
    "    \n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    all_click_df[feature] = lbe.fit_transform(all_click_df[feature])\n",
    "    feature_max_idx[feature] = all_click_df[feature].max() + 1\n",
    "    \n",
    "\n",
    "# extract user and item profile\n",
    "user_profile = all_click_df[[\"user_id\"]].drop_duplicates('user_id')\n",
    "item_profile = all_click_df[[\"click_article_id\"]].drop_duplicates('click_article_id')  \n",
    "user_index_2_rawid = dict(zip(user_profile['user_id'], user_profile_['user_id']))\n",
    "item_index_2_rawid = dict(zip(item_profile['click_article_id'], item_profile_['click_article_id']))\n",
    " \n",
    "train_set, test_set = gen_data_set(all_click_df, 0)\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "X_train = {\n",
    "    \"user_id\": train_model_input[\"user_id\"],\n",
    "    \"click_article_id\": train_model_input[\"click_article_id\"],\n",
    "    \"hist_article_id\": train_model_input[\"hist_article_id\"],\n",
    "    \"hist_len\": train_model_input[\"hist_len\"]\n",
    "}\n",
    "y_train = train_label.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fef1f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input user_id: (1149673,)\n",
      "Input click_article_id: (1149673,)\n",
      "Input hist_article_id: (1149673, 10)\n",
      "Input hist_len: (1149673,)\n",
      "35380\n",
      "Concatenated shape: (None, 48)\n",
      "Shape of hist_article_id: (1149673, 1)\n"
     ]
    }
   ],
   "source": [
    "for key, value in X_train.items():\n",
    "    print(f\"Input {key}: {value.shape}\")\n",
    "print(all_click_df[\"click_article_id\"].nunique())\n",
    "print(\"Concatenated shape:\", concat.shape)  # Expected: (batch_size, 48)\n",
    "print(f\"Shape of hist_article_id: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be59354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7186/7186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0157 - val_accuracy: 1.0000 - val_loss: 8.4651e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x36ea901d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97ac18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve embedding layers\n",
    "user_embedding_layer = model.get_layer(\"user_embedding\")\n",
    "item_embedding_layer = model.get_layer(\"item_embedding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6452f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embedding matrix shape: (250000, 16)\n",
      "Item embedding matrix shape: (35380, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35380"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the embedding weights\n",
    "# User embeddings\n",
    "user_embs = user_embedding_layer.get_weights()[0]\n",
    "\n",
    "# Item embeddings\n",
    "item_embs = item_embedding_layer.get_weights()[0]\n",
    "\n",
    "print(\"User embedding matrix shape:\", user_embs.shape)\n",
    "print(\"Item embedding matrix shape:\", item_embs.shape)\n",
    "#test_user_model_input = test_model_input\n",
    "#all_item_model_input = {\"click_article_id\": item_profile['click_article_id'].values}\n",
    "# normalization before saving to local file\n",
    "user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n",
    " # convert Embedding to dict for fast reference\n",
    "raw_user_id_emb_dict = {user_index_2_rawid[k]: \\\n",
    "                                v for k, v in zip(user_profile['user_id'], user_embs)}\n",
    "raw_item_id_emb_dict = {item_index_2_rawid[k]: \\\n",
    "                                v for k, v in zip(item_profile['click_article_id'], item_embs)}\n",
    "# save Embedding to local\n",
    "pickle.dump(raw_user_id_emb_dict, open(save_path + 'user_youtube_emb.pkl', 'wb'))\n",
    "pickle.dump(raw_item_id_emb_dict, open(save_path + 'item_youtube_emb.pkl', 'wb'))\n",
    "        \n",
    "len(item_index_2_rawid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb8ec7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch candidate generation, using Approximate Nearest Neighbours for efficient search\n",
    "# Build a FAISS index\n",
    "embedding_dim = item_embs.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(embedding_dim)  # Inner product similarity\n",
    "faiss_index.add(item_embs)          # Add item embeddings\n",
    "\n",
    "# Perform batch search for top-k items\n",
    "batch_size = user_embs.shape[0]\n",
    "top_k = 10\n",
    "sim, idx = faiss_index.search(user_embs, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd8aeebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35155, 22393, 21950,  1695, 22980,  5818, 28566, 30313, 22728,\n",
       "       25791])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f047be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_model_input['user_id'], sim, idx)):\n",
    "    target_raw_id = user_index_2_rawid[target_idx]\n",
    "    #Iterates through the top-k recommended items (excluding the item itself).\n",
    "    for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]): \n",
    "        rele_raw_id = item_index_2_rawid[rele_idx]\n",
    "        user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, {})\\\n",
    "                                                                    .get(rele_raw_id, 0) + sim_value\n",
    "            \n",
    "user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in user_recall_items_dict.items()}\n",
    "# ranking recalled items\n",
    "\n",
    "pickle.dump(user_recall_items_dict, open(save_path + 'youtube_u2i_dict.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0cb9ba40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for target_idx, _, rele_idx_list in zip(test_model_input['user_id'], sim, idx):\n",
    "    for rele_idx in rele_idx_list:\n",
    "        if rele_idx not in item_index_2_rawid:\n",
    "            print(f\"Missing index: {rele_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f17c1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_multi_recall_dict['youtubednn_recall'] = user_recall_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32a5bd",
   "metadata": {},
   "source": [
    "## Cold start recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90ec54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 250000/250000 [00:42<00:00, 5823.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 150\n",
    "recall_item_num = 100 # 稍微召回多一点文章，便于后续的规则筛选\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, \n",
    "                                                        recall_item_num, item_topk_click,item_created_time_dict, emb_i2i_sim)\n",
    "pickle.dump(user_recall_items_dict, open(save_path + 'cold_start_items_raw_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "882c0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_click_article_ids_set(all_click_df):\n",
    "    return set(all_click_df.click_article_id.values)\n",
    "\n",
    "def cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                     user_last_item_created_time_dict, item_type_dict, item_words_dict, \n",
    "                     item_created_time_dict, click_article_ids_set, recall_item_num):\n",
    "    \"\"\"\n",
    "        冷启动的情况下召回一些文章\n",
    "        :param user_recall_items_dict: 基于内容embedding相似性召回来的很多文章， 字典， {user1: [item1, item2, ..], }\n",
    "        :param user_hist_item_typs_dict: 字典， 用户点击的文章的主题映射\n",
    "        :param user_hist_item_words_dict: 字典， 用户点击的历史文章的字数映射\n",
    "        :param user_last_item_created_time_idct: 字典，用户点击的历史文章创建时间映射\n",
    "        :param item_tpye_idct: 字典，文章主题映射\n",
    "        :param item_words_dict: 字典，文章字数映射\n",
    "        :param item_created_time_dict: 字典， 文章创建时间映射\n",
    "        :param click_article_ids_set: 集合，用户点击过得文章, 也就是日志里面出现过的文章\n",
    "        :param recall_item_num: 召回文章的数量， 这个指的是没有出现在日志里面的文章数量\n",
    "    \"\"\"\n",
    "    \n",
    "    cold_start_user_items_dict = {}\n",
    "    for user, item_list in tqdm(user_recall_items_dict.items()):\n",
    "        cold_start_user_items_dict.setdefault(user, [])\n",
    "        for item, score in item_list:\n",
    "            # getting historical data\n",
    "            hist_item_type_set = user_hist_item_typs_dict[user]\n",
    "            hist_mean_words = user_hist_item_words_dict[user]\n",
    "            hist_last_item_created_time = user_last_item_created_time_dict[user]\n",
    "            hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)\n",
    "            \n",
    "            # getting the info of current recalled article\n",
    "            curr_item_type = item_type_dict[item]\n",
    "            curr_item_words = item_words_dict[item]\n",
    "            curr_item_created_time = item_created_time_dict[item]\n",
    "            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)\n",
    "\n",
    "            # 首先，文章不能出现在用户的历史点击中， 然后根据文章主题，文章单词数，文章创建时间进行筛选\n",
    "            if curr_item_type not in hist_item_type_set or \\\n",
    "                item in click_article_ids_set or \\\n",
    "                abs(curr_item_words - hist_mean_words) > 200 or \\\n",
    "                abs((curr_item_created_time - hist_last_item_created_time).days) > 90: \n",
    "                continue\n",
    "                \n",
    "            cold_start_user_items_dict[user].append((item, score))      # {user1: [(item1, score1), (item2, score2)..]...}\n",
    "        print(f\"Processed {user} - {len(cold_start_user_items_dict[user])} items recommended.\")\n",
    "\n",
    "    # 需要控制一下冷启动召回的数量\n",
    "    cold_start_user_items_dict = {k: sorted(v, key=lambda x:x[1], reverse=True)[:recall_item_num] \\\n",
    "                                  for k, v in cold_start_user_items_dict.items()}\n",
    "    \n",
    "    pickle.dump(cold_start_user_items_dict, open(save_path + 'cold_start_user_items_dict.pkl', 'wb'))\n",
    "    \n",
    "    return cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8c99fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df_ = all_click_df.copy()\n",
    "all_click_df_ = all_click_df_.merge(item_info_df, how='left', on='click_article_id')\n",
    "user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)\n",
    "click_article_ids_set = get_click_article_ids_set(all_click_df)\n",
    "\n",
    "cold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \\\n",
    "                                              item_created_time_dict, click_article_ids_set, recall_item_num)\n",
    "\n",
    "user_multi_recall_dict['cold_start_recall'] = cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a932763",
   "metadata": {},
   "source": [
    "# Combine recall results from multiple ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "57c55fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25):\n",
    "    final_recall_items_dict = {}\n",
    "    \n",
    "    # normalizing user recall articles for easy aggregation\n",
    "    def norm_user_recall_items_sim(sorted_item_list):\n",
    "        # 如果冷启动中没有文章或者只有一篇文章，直接返回，出现这种情况的原因可能是冷启动召回的文章数量太少了，\n",
    "        # 基于规则筛选之后就没有文章了, 这里还可以做一些其他的策略性的筛选\n",
    "        if len(sorted_item_list) < 2:\n",
    "            return sorted_item_list\n",
    "        \n",
    "        min_sim = sorted_item_list[-1][1]\n",
    "        max_sim = sorted_item_list[0][1]\n",
    "        \n",
    "        norm_sorted_item_list = []\n",
    "        for item, score in sorted_item_list:\n",
    "            if max_sim > 0:\n",
    "                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0\n",
    "            else:\n",
    "                norm_score = 0.0\n",
    "            norm_sorted_item_list.append((item, norm_score))\n",
    "            \n",
    "        return norm_sorted_item_list\n",
    "    \n",
    "    print('multi recall merging...')\n",
    "    for method, user_recall_items in tqdm(user_multi_recall_dict.items()):\n",
    "        print(method + '...')\n",
    "        # set a weight to result from each recall method\n",
    "        if weight_dict == None:\n",
    "            recall_method_weight = 1\n",
    "        else:\n",
    "            recall_method_weight = weight_dict[method]\n",
    "        \n",
    "        for user_id, sorted_item_list in user_recall_items.items(): # 进行归一化\n",
    "            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)\n",
    "        \n",
    "        for user_id, sorted_item_list in user_recall_items.items():\n",
    "            # print('user_id')\n",
    "            final_recall_items_dict.setdefault(user_id, {})\n",
    "            for item, score in sorted_item_list:\n",
    "                final_recall_items_dict[user_id].setdefault(item, 0)\n",
    "                final_recall_items_dict[user_id][item] += recall_method_weight * score  \n",
    "    \n",
    "    final_recall_items_dict_rank = {}\n",
    "    for user, recall_item_dict in final_recall_items_dict.items():\n",
    "        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # saving to local\n",
    "    pickle.dump(final_recall_items_dict, open(os.path.join(save_path, 'final_recall_items_dict.pkl'),'wb'))\n",
    "\n",
    "    return final_recall_items_dict_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3e3799ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights for similarity scores computed from multiple methods\n",
    "weight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n",
    "               'embedding_sim_item_recall': 1.0,\n",
    "               'youtubednn_recall': 1.0,\n",
    "               'cold_start_recall': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "25af559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking the top k recalled items\n",
    "final_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, topk=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92133ec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_recall_items_dict_rank\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'info'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a4c039e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(user_recall_items_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742ed06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7de9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
